# CS4170_FA2025_A03_Jenkins


Started by making the vector matrix multiplication algorithm for the serial base line. My first issue is that the algorithm is running faster than I would like. I am running a 10x10 matrix and measuring in milliseconds, and I ran it on OSC, and my local VSCode, both resulting in below 0 milliseconds. I need to decide if I should measure in nanoseconds or increase the matrix size. I may end up doing both. But as of now the program works and I can get a good baseline to base my future metrics on for calculations. 

Now I've got all the code working and giving a good metric. I changed the matrix to be generated rather than having a matrix defined in the main. I've also changed the timer to be the MPI form because it is more accurate for using MPI. I added a barrier for the timer in the parallelized function to allow all the processes to be at the same point before starting the timers and then computing. The timers in both functions do not time the generation or distribution of rows. I thought this to be the best way to only see how the computation time has changed for using MPI vs serial. During the process of making the program I would output the result vector of the serial and parallel implementations in the results file to ensure the data was coming out correct each run. I now only show the first 2  elements of each process in the vector to make sure the computations are correct. I've removed the printing of the result vector as the size is too large to realistically print. 

For the parallelized version of vector multiplication I used a similar approach to the vector sum example we have seen in class. I defined my variables, then I generated the matrix and vector. Then I decided to divide the work between processes by assigning a chunk of rows from the matrix to each process. I performed this by using counts, and displacements to calculate the starting index and chunk size for each process. Since the matrix is essentially a 2D array, I needed to flatten it for the MPI functions to work properly with regards to the Scatterv. So after flattening the matrix I have to modify the displacements to correlate to the flattened matrix to ensure the correct indexes. I then scattered the flat vector, and broadcasted the x-vector since the processes won't need to modify it. To make sure the timer was as accurate as possible I added a barrier to make all the processes start their computation at the same time. Computation takes place then the master uses the Gatherv function to gather all the results for the processes. Then we stop the timer because after the result vector is filled that is what I considered the end of computation. Each process then calculates their elapsed time and I used reduce to combine all the times and only use the largest to make sure I had the full timing. Then write the numProcs, problemsize, and time to the results file. 
I have also found that if I try 100,000 as input size I run out of memory, so I used 25,000 as input after testing.

Benchmarking the code shows that the code runs very well. I know that I was running a 25,000 x 25,000 matrix and running that on 1 process is a long task just by itself. But with this large of a dataset we can clearly see how much faster just 2 processors are compared to one, and even faster the more I added. I did run into the out of memory error at some points while making this code and testing it. The metrics aren’t the best representation because for the efficiency and the Karp-Flat I can tell that the fact that just adding one other processor cut the computation time down by over a second really skewed the resulting data. 

I learned a lot about working with MPI and also working in larger data sets. MPI isn’t hard to learn either, it was easy to implement and easy to follow. The hardest part was making sure I knew what I was passing between processors and where I was storing them. Another thing that I have to learn is the MPI functions parameters, I had to keep looking up the parameters to make sure I had them right before hand. The only bad part for me was that I had encountered a few MPI hangs and had to find out what was happening by working it through step by step. But I was able to work through all the bugs that I created and now I got it working great. I would advise someone doing this assignment to know the whole program before you try writing. I had started with a constant vector defined in the main, and when I needed to change to a generated vector I had to go through both the serial and parallel and pick through all the little things to change them for generating a vector. 
